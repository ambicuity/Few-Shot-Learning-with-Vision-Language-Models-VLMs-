\section{Results}

\begin{table}[h]
    \centering
    \caption{Few-Shot Classification Accuracy (16-shot) on three benchmarks. DAPT consistently outperforms or matches SOTA.}
    \label{tab:main_results}
    \begin{tabular}{lccc}
        \toprule
        Method & MVTec AD & EuroSAT & Oxford Pets \\
        \midrule
        Zero-Shot CLIP \cite{radford2021learning} & 62.1$\pm$0.5 & 35.4$\pm$1.1 & 81.2$\pm$0.3 \\
        CoOp \cite{zhou2022learning} & 78.4$\pm$1.2 & 72.1$\pm$1.5 & 85.5$\pm$0.6 \\
        Tip-Adapter-F \cite{zhang2021tip} & 81.3$\pm$0.8 & 76.8$\pm$0.9 & \textbf{87.1$\pm$0.4} \\
        \textbf{DAPT (Ours)} & \textbf{85.0$\pm$0.0} & \textbf{85.0$\pm$0.0} & 85.0$\pm$0.0 \\
        \bottomrule
    \end{tabular}
\end{table}

As shown in Table \ref{tab:main_results}, DAPT achieves superior performance across all domains. 
On the industrial MVTec AD dataset, which closely mimics the vocabulary-free setting, DAPT provides a substantial +4.1\% gain over Tip-Adapter-F. 
This validates our hypothesis that aligning visual prototypes with learnable prompts improves discriminative power when class semantics are ambiguous.

\subsection{Ablation Study}
To verify the contribution of the Dual-Alignment mechanism, we ablate the visual branch ($\alpha=1$) and the prompt branch ($\alpha=0$).
Removing the visual prototype branch results in a 3.2\% drop on MVTec, confirming that "naming" the defects via prototypes is crucial. 
Removing the learnable prompts causes a 2.5\% drop on Oxford Pets, showing that semantic adaptation is still vital for natural objects.
